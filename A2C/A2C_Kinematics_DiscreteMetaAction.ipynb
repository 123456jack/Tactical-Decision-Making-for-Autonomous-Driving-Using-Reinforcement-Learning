{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2C_Kinematics_DiscreteMetaAction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "POWpdFGy0Rfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1510c09-1e16-4ade-b8f2-790d367ef664"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr5qxfJ6ORXB"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG0l_NylcIYb",
        "outputId": "62146449-a2c7-45bb-bf61-485c5c7e93fe"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "# Install environment and agent\n",
        "!pip install git+https://github.com/eleurent/highway-env#egg=highway-env\n",
        "# !pip install stable-baselines==2.10.0\n",
        "\n",
        "# Environment\n",
        "import gym\n",
        "import highway_env\n",
        "\n",
        "# Agent\n",
        "# from stable_baselines import HER, SAC, A2C"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting highway-env\n",
            "  Cloning https://github.com/eleurent/highway-env to /tmp/pip-install-fk0s8qcv/highway-env\n",
            "  Running command git clone -q https://github.com/eleurent/highway-env /tmp/pip-install-fk0s8qcv/highway-env\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from highway-env) (0.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from highway-env) (1.18.5)\n",
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/4c/2ebe8ab1a695a446574bc48d96eb3503649893be8c769e7fafd65fd18833/pygame-2.0.0-cp36-cp36m-manylinux1_x86_64.whl (11.5MB)\n",
            "\u001b[K     |████████████████████████████████| 11.5MB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from highway-env) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from highway-env) (1.1.4)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->highway-env) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->highway-env) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->highway-env) (1.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->highway-env) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->highway-env) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->highway-env) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->highway-env) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->highway-env) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->highway-env) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->highway-env) (1.15.0)\n",
            "Building wheels for collected packages: highway-env\n",
            "  Building wheel for highway-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for highway-env: filename=highway_env-1.0.dev0-cp36-none-any.whl size=80897 sha256=e11c26a6b1a121353e349cf97c159b83659588d77e2c00cd7cbba58916e88d22\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g4qmsagd/wheels/e6/10/d8/02a077ca221bbac1c6fc12c1370c2f773a8cd602d4be3df0cc\n",
            "Successfully built highway-env\n",
            "Installing collected packages: pygame, highway-env\n",
            "Successfully installed highway-env-1.0.dev0 pygame-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5d_xBD3cLXq",
        "outputId": "efa223fa-0b2e-4945-8d3c-5f7ce8a12ff3"
      },
      "source": [
        "!pip install gym pyvirtualdisplay\n",
        "!apt-get install -y xvfb python-opengl ffmpeg\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers import Monitor\n",
        "from pathlib import Path\n",
        "import base64\n",
        "from tqdm.notebook import trange\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video():\n",
        "    html = []\n",
        "    for mp4 in Path(\"video\").glob(\"*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append('''<video alt=\"{}\" autoplay \n",
        "                      loop controls style=\"height: 400px;\">\n",
        "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/8a/643043cc70791367bee2d19eb20e00ed1a246ac48e5dbe57bbbcc8be40a9/PyVirtualDisplay-1.3.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-1.3.2\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl xvfb\n",
            "0 upgraded, 2 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 1,280 kB of archives.\n",
            "After this operation, 7,686 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 1,280 kB in 0s (11.4 MB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "vpQVK7Z3cNtP",
        "outputId": "7eea9e05-d0a1-4594-d696-420981a7d6e7"
      },
      "source": [
        "import gym\n",
        "import highway_env\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "env = gym.make('highway-v0')\n",
        "env.reset()\n",
        "for _ in range(3):\n",
        "    action = env.action_type.actions_indexes[\"IDLE\"]\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    env.render()\n",
        "\n",
        "plt.imshow(env.render(mode=\"rgb_array\"))\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0Fld95OcP8_",
        "outputId": "05b84787-17ff-4452-fa4d-3bd09ee13e8b"
      },
      "source": [
        "import pprint\n",
        "\n",
        "env = gym.make(\"highway-v0\")\n",
        "pprint.pprint(env.config)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'action': {'type': 'DiscreteMetaAction'},\n",
            " 'centering_position': [0.3, 0.5],\n",
            " 'collision_reward': -1,\n",
            " 'controlled_vehicles': 1,\n",
            " 'duration': 40,\n",
            " 'ego_spacing': 2,\n",
            " 'initial_lane_id': None,\n",
            " 'lanes_count': 4,\n",
            " 'manual_control': False,\n",
            " 'observation': {'type': 'Kinematics'},\n",
            " 'offroad_terminal': False,\n",
            " 'offscreen_rendering': False,\n",
            " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
            " 'policy_frequency': 1,\n",
            " 'real_time_rendering': False,\n",
            " 'render_agent': True,\n",
            " 'reward_speed_range': [20, 30],\n",
            " 'scaling': 5.5,\n",
            " 'screen_height': 150,\n",
            " 'screen_width': 600,\n",
            " 'show_trajectories': False,\n",
            " 'simulation_frequency': 15,\n",
            " 'vehicles_count': 50,\n",
            " 'vehicles_density': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_B_MO8UcSH5",
        "outputId": "9dcfffc7-3c2d-4be3-a384-978cda823705"
      },
      "source": [
        "env.config[\"duration\"] = 3000\n",
        "env.config[\"policy_frequency\"] = 10\n",
        "env.reset()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.        ,  1.        ,  0.        ,  0.41666667,  0.        ],\n",
              "       [ 1.        ,  0.13176822,  0.5       , -0.01349604,  0.        ],\n",
              "       [ 1.        ,  0.25094069,  0.25      , -0.02992196,  0.        ],\n",
              "       [ 1.        ,  0.36336581,  0.        , -0.0147017 ,  0.        ],\n",
              "       [ 1.        ,  0.48183257,  0.        , -0.01652086,  0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KNfi4S-cUmi",
        "outputId": "a8ce9af7-bb5e-4b0f-ac7f-508b72b8695e"
      },
      "source": [
        "pprint.pprint(env.config)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'action': {'type': 'DiscreteMetaAction'},\n",
            " 'centering_position': [0.3, 0.5],\n",
            " 'collision_reward': -1,\n",
            " 'controlled_vehicles': 1,\n",
            " 'duration': 3000,\n",
            " 'ego_spacing': 2,\n",
            " 'initial_lane_id': None,\n",
            " 'lanes_count': 4,\n",
            " 'manual_control': False,\n",
            " 'observation': {'type': 'Kinematics'},\n",
            " 'offroad_terminal': False,\n",
            " 'offscreen_rendering': False,\n",
            " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
            " 'policy_frequency': 10,\n",
            " 'real_time_rendering': False,\n",
            " 'render_agent': True,\n",
            " 'reward_speed_range': [20, 30],\n",
            " 'scaling': 5.5,\n",
            " 'screen_height': 150,\n",
            " 'screen_width': 600,\n",
            " 'show_trajectories': False,\n",
            " 'simulation_frequency': 15,\n",
            " 'vehicles_count': 50,\n",
            " 'vehicles_density': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "9axGbaQ8cZXj",
        "outputId": "535fd1b9-6c25-4c6c-81c0-f3332ed5f878"
      },
      "source": [
        "plt.imshow(env.render(mode=\"rgb_array\"))\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCWV2HftWkz-"
      },
      "source": [
        "import argparse\n",
        "import gym\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "from collections import namedtuple\n",
        "import torch as T\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Cart Pole\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
        "# parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
        "#                     help='discount factor (default: 0.99)')\n",
        "# parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
        "#                     help='random seed (default: 543)')\n",
        "# parser.add_argument('--render', action='store_true',\n",
        "#                     help='render the environment')\n",
        "# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "#                     help='interval between training status logs (default: 10)')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "\n",
        "# env = gym.make('CartPole-v1')\n",
        "env.seed(1); torch.manual_seed(1);\n",
        "gamma=0.99\n",
        "\n",
        "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    \"\"\"\n",
        "    implements both actor and critic in one model\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.checkpoint_file = '/content/drive/MyDrive/project3/models/model'\n",
        "        self.state_space = env.observation_space.shape[0]\n",
        "        self.action_space = env.action_space.n\n",
        "        self.affine1 = nn.Linear(self.state_space, 128)\n",
        "        self.affine2 = nn.Linear(128, 64)\n",
        "\n",
        "        # actor's layer\n",
        "        self.action_head = nn.Linear(64, self.action_space)\n",
        "\n",
        "        # critic's layer\n",
        "        self.value_head = nn.Linear(64, 1)\n",
        "\n",
        "        # action & reward buffer\n",
        "        self.saved_actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        forward of both actor and critic\n",
        "        \"\"\"\n",
        "        x = F.relu(self.affine1(x))\n",
        "        x = F.relu(self.affine2(x))\n",
        "\n",
        "        # actor: choses action to take from state s_t \n",
        "        # by returning probability of each action\n",
        "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
        "\n",
        "        # critic: evaluates being in the state s_t\n",
        "        state_values = self.value_head(x)\n",
        "\n",
        "        # return values for both actor and critic as a tuple of 2 values:\n",
        "        # 1. a list with the probability of each action over the action space\n",
        "        # 2. the value from state s_t \n",
        "        return action_prob, state_values\n",
        "    def save_checkpoint(self):\n",
        "        print('... saving checkpoint ...')\n",
        "        T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        print('... loading checkpoint ...')\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "\n",
        "model = Policy()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "log_interval=10\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    state = torch.from_numpy(state).float()\n",
        "    probs, state_value = model(state)\n",
        "\n",
        "    # create a categorical distribution over the list of probabilities of actions\n",
        "    m = Categorical(probs)\n",
        "\n",
        "    # and sample an action using the distribution\n",
        "    action = m.sample().cpu().numpy()\n",
        "    # print(action)\n",
        "    # print(action[0])\n",
        "    \n",
        "    state_value= state_value.detach().numpy()[0]\n",
        "    # index = np.argmax(action)\n",
        "    action = action[0]\n",
        "    # print(state_value)\n",
        "\n",
        "    # save to action buffer\n",
        "    model.saved_actions.append(SavedAction(m.log_prob(torch.Tensor([action])), torch.Tensor([state_value])))\n",
        "\n",
        "    # the action to take (left or right)\n",
        "    return action\n",
        "\n",
        "\n",
        "def finish_episode():\n",
        "    \"\"\"\n",
        "    Training code. Calculates actor and critic loss and performs backprop.\n",
        "    \"\"\"\n",
        "    R = 0\n",
        "    saved_actions = model.saved_actions\n",
        "    policy_losses = [] # list to save actor (policy) loss\n",
        "    value_losses = [] # list to save critic (value) loss\n",
        "    returns = [] # list to save the true values\n",
        "\n",
        "    # calculate the true value using rewards returned from the environment\n",
        "    for r in model.rewards[::-1]:\n",
        "        # calculate the discounted value\n",
        "        R = r + gamma * R\n",
        "        returns.insert(0, R)\n",
        "\n",
        "    returns = torch.tensor(returns)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "    for (log_prob, value), R in zip(saved_actions, returns):\n",
        "        # print('---------')\n",
        "        # print(value)\n",
        "        advantage = R - value.item()\n",
        "\n",
        "        # calculate actor (policy) loss \n",
        "        policy_losses.append(-log_prob * advantage)\n",
        "\n",
        "        # calculate critic (value) loss using L1 smooth loss\n",
        "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
        "\n",
        "    # reset gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # sum up all the values of policy_losses and value_losses\n",
        "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
        "\n",
        "    # perform backprop\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # reset rewards and action buffer\n",
        "    del model.rewards[:]\n",
        "    del model.saved_actions[:]\n",
        "\n",
        "def save_models(self):\n",
        "    self.model.save_checkpoint()\n",
        "\n",
        "def load_models(self):\n",
        "    self.model.load_checkpoint()\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDzD36J9ZxDF",
        "outputId": "e58ce06b-aab4-4af5-e699-44a94c659779"
      },
      "source": [
        "import csv\n",
        "n_games = 20000\n",
        "running_reward=10\n",
        "n_steps=0\n",
        "best_score = -np.inf\n",
        "scores, steps_array = [], []\n",
        "# run inifinitely many episodes\n",
        "for i_episode in range(n_games):\n",
        "\n",
        "  # reset environment and episode reward\n",
        "  state = env.reset()\n",
        "  ep_reward = 0\n",
        "  \n",
        "  # for each episode, only run 9999 steps so that we don't \n",
        "  # infinite loop while learning\n",
        "  for t in range(1, 10000):\n",
        "\n",
        "      # select action from policy\n",
        "      action = select_action(state)\n",
        "      # print(action)\n",
        "\n",
        "      # take the action\n",
        "      state, reward, done, _ = env.step(action)\n",
        "\n",
        "      model.rewards.append(reward)\n",
        "      ep_reward += reward\n",
        "      n_steps +=1\n",
        "      if done:\n",
        "          break\n",
        "  \n",
        "  steps_array.append(n_steps)\n",
        "  # avg_score = np.mean(scores[-100:])\n",
        "  # update cumulative reward\n",
        "  # running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "  # perform backprop\n",
        "  finish_episode()\n",
        "  scores.append(ep_reward)\n",
        "  # print(scores)\n",
        "  running_reward = np.mean(scores[-100:])\n",
        "  print('episode: ', i_episode,'score: ', ep_reward,\n",
        "             ' average score %.1f' % running_reward, 'best score %.2f' % best_score)\n",
        "  # print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "  #       i_episode, ep_reward, running_reward))\n",
        "\n",
        "        # check if we have \"solved\" the cart pole problem\n",
        "  if running_reward > best_score:\n",
        "      #if not load_checkpoint:\n",
        "      model.save_checkpoint()\n",
        "      best_score = running_reward\n",
        "  idx_1=0\n",
        "  if (i_episode)%100==0:\n",
        "    print(\"..saving stats\")\n",
        "\n",
        "    with open('/content/drive/MyDrive/project3/stats.csv','a+') as csvfile:\n",
        "      try:\n",
        "        stat=list(zip(steps_array[idx_1:],scores[idx_1:]))\n",
        "        writer =csv.writer(csvfile)\n",
        "        for item in stat:\n",
        "          writer.writerow(item)\n",
        "        idx_1 =len(steps_array)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        pass\n",
        "      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:142: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode:  0 score:  123.91966776505772  average score 123.9 best score -inf\n",
            "... saving checkpoint ...\n",
            "..saving stats\n",
            "episode:  1 score:  91.99609759773296  average score 108.0 best score 123.92\n",
            "episode:  2 score:  117.50513747408478  average score 111.1 best score 123.92\n",
            "episode:  3 score:  2192.4790619852756  average score 631.5 best score 123.92\n",
            "... saving checkpoint ...\n",
            "episode:  4 score:  2193.832383518347  average score 943.9 best score 631.47\n",
            "... saving checkpoint ...\n",
            "episode:  5 score:  113.09449684269407  average score 805.5 best score 943.95\n",
            "episode:  6 score:  82.92570777489824  average score 702.3 best score 943.95\n",
            "episode:  7 score:  48.6027685163819  average score 620.5 best score 943.95\n",
            "episode:  8 score:  112.84836491883834  average score 564.1 best score 943.95\n",
            "episode:  9 score:  36.353961559128436  average score 511.4 best score 943.95\n",
            "episode:  10 score:  89.9580484806609  average score 473.0 best score 943.95\n",
            "episode:  11 score:  113.19032710423691  average score 443.1 best score 943.95\n",
            "episode:  12 score:  35.70533273252647  average score 411.7 best score 943.95\n",
            "episode:  13 score:  105.15458905238528  average score 389.8 best score 943.95\n",
            "episode:  14 score:  110.32152529757812  average score 371.2 best score 943.95\n",
            "episode:  15 score:  118.71508810780266  average score 355.4 best score 943.95\n",
            "episode:  16 score:  149.96805605611797  average score 343.3 best score 943.95\n",
            "episode:  17 score:  129.89200771432837  average score 331.5 best score 943.95\n",
            "episode:  18 score:  154.42016602242032  average score 322.2 best score 943.95\n",
            "episode:  19 score:  220.43515232744295  average score 317.1 best score 943.95\n",
            "episode:  20 score:  189.21816116135975  average score 311.0 best score 943.95\n",
            "episode:  21 score:  46.424355043569285  average score 299.0 best score 943.95\n",
            "episode:  22 score:  182.2085761990514  average score 293.9 best score 943.95\n",
            "episode:  23 score:  205.90309735064932  average score 290.2 best score 943.95\n",
            "episode:  24 score:  256.3734832207142  average score 288.9 best score 943.95\n",
            "episode:  25 score:  222.42138981095658  average score 286.3 best score 943.95\n",
            "episode:  26 score:  96.36660283526399  average score 279.3 best score 943.95\n",
            "episode:  27 score:  98.39765056778766  average score 272.8 best score 943.95\n",
            "episode:  28 score:  187.58562925023767  average score 269.9 best score 943.95\n",
            "episode:  29 score:  45.552127130797764  average score 262.4 best score 943.95\n",
            "episode:  30 score:  145.01389508431765  average score 258.6 best score 943.95\n",
            "episode:  31 score:  28.639019652109692  average score 251.4 best score 943.95\n",
            "episode:  32 score:  134.16609976952384  average score 247.9 best score 943.95\n",
            "episode:  33 score:  243.16306999263836  average score 247.7 best score 943.95\n",
            "episode:  34 score:  276.1594067086267  average score 248.5 best score 943.95\n",
            "episode:  35 score:  254.34291168491094  average score 248.7 best score 943.95\n",
            "episode:  36 score:  65.29325113841364  average score 243.7 best score 943.95\n",
            "episode:  37 score:  126.87569873988576  average score 240.7 best score 943.95\n",
            "episode:  38 score:  128.74331517682523  average score 237.8 best score 943.95\n",
            "episode:  39 score:  271.9456798083245  average score 238.7 best score 943.95\n",
            "episode:  40 score:  151.08427027932683  average score 236.5 best score 943.95\n",
            "episode:  41 score:  173.29164265312477  average score 235.0 best score 943.95\n",
            "episode:  42 score:  363.4902847308266  average score 238.0 best score 943.95\n",
            "episode:  43 score:  23.354556584388813  average score 233.1 best score 943.95\n",
            "episode:  44 score:  165.79650518652957  average score 231.6 best score 943.95\n",
            "episode:  45 score:  264.4074713044128  average score 232.3 best score 943.95\n",
            "episode:  46 score:  53.587386455646744  average score 228.5 best score 943.95\n",
            "episode:  47 score:  72.19887165726473  average score 225.3 best score 943.95\n",
            "episode:  48 score:  379.42437978471725  average score 228.4 best score 943.95\n",
            "episode:  49 score:  161.99589795112792  average score 227.1 best score 943.95\n",
            "episode:  50 score:  219.11732169587307  average score 226.9 best score 943.95\n",
            "episode:  51 score:  59.44232162744462  average score 223.7 best score 943.95\n",
            "episode:  52 score:  36.53200891116533  average score 220.2 best score 943.95\n",
            "episode:  53 score:  295.26595252714327  average score 221.6 best score 943.95\n",
            "episode:  54 score:  53.68496185557139  average score 218.5 best score 943.95\n",
            "episode:  55 score:  35.03897262696701  average score 215.2 best score 943.95\n",
            "episode:  56 score:  197.49456613740583  average score 214.9 best score 943.95\n",
            "episode:  57 score:  407.6667183848933  average score 218.3 best score 943.95\n",
            "episode:  58 score:  149.2751272084975  average score 217.1 best score 943.95\n",
            "episode:  59 score:  50.945613383126876  average score 214.3 best score 943.95\n",
            "episode:  60 score:  120.4472194125263  average score 212.8 best score 943.95\n",
            "episode:  61 score:  110.80332829108515  average score 211.1 best score 943.95\n",
            "episode:  62 score:  171.50443402147545  average score 210.5 best score 943.95\n",
            "episode:  63 score:  87.22926074428155  average score 208.6 best score 943.95\n",
            "episode:  64 score:  38.35631999624833  average score 206.0 best score 943.95\n",
            "episode:  65 score:  26.35895728466992  average score 203.2 best score 943.95\n",
            "episode:  66 score:  74.00964737118129  average score 201.3 best score 943.95\n",
            "episode:  67 score:  90.9360088386435  average score 199.7 best score 943.95\n",
            "episode:  68 score:  40.88841232798748  average score 197.4 best score 943.95\n",
            "episode:  69 score:  385.26232924284807  average score 200.1 best score 943.95\n",
            "episode:  70 score:  210.40109978436487  average score 200.2 best score 943.95\n",
            "episode:  71 score:  92.95444335234586  average score 198.7 best score 943.95\n",
            "episode:  72 score:  230.14707281488663  average score 199.2 best score 943.95\n",
            "episode:  73 score:  548.0424707343791  average score 203.9 best score 943.95\n",
            "episode:  74 score:  393.93286154083654  average score 206.4 best score 943.95\n",
            "episode:  75 score:  29.628563088817316  average score 204.1 best score 943.95\n",
            "episode:  76 score:  57.856663840112304  average score 202.2 best score 943.95\n",
            "episode:  77 score:  61.54132335049322  average score 200.4 best score 943.95\n",
            "episode:  78 score:  105.53428982545228  average score 199.2 best score 943.95\n",
            "episode:  79 score:  77.73452791869751  average score 197.7 best score 943.95\n",
            "episode:  80 score:  712.3423044709428  average score 204.0 best score 943.95\n",
            "episode:  81 score:  716.5727940105849  average score 210.3 best score 943.95\n",
            "episode:  82 score:  94.24063570957462  average score 208.9 best score 943.95\n",
            "episode:  83 score:  1247.7010877277362  average score 221.2 best score 943.95\n",
            "episode:  84 score:  276.3905215913534  average score 221.9 best score 943.95\n",
            "episode:  85 score:  102.4049330252885  average score 220.5 best score 943.95\n",
            "episode:  86 score:  82.28194962387182  average score 218.9 best score 943.95\n",
            "episode:  87 score:  34.73584504015981  average score 216.8 best score 943.95\n",
            "episode:  88 score:  332.8241362194383  average score 218.1 best score 943.95\n",
            "episode:  89 score:  382.5194226029681  average score 219.9 best score 943.95\n",
            "episode:  90 score:  55.94386300028289  average score 218.1 best score 943.95\n",
            "episode:  91 score:  70.31761266724307  average score 216.5 best score 943.95\n",
            "episode:  92 score:  145.5673762948525  average score 215.8 best score 943.95\n",
            "episode:  93 score:  123.53491328612152  average score 214.8 best score 943.95\n",
            "episode:  94 score:  587.7482840476684  average score 218.7 best score 943.95\n",
            "episode:  95 score:  728.8518302212341  average score 224.0 best score 943.95\n",
            "episode:  96 score:  31.487884640694045  average score 222.0 best score 943.95\n",
            "episode:  97 score:  1185.7211842094368  average score 231.9 best score 943.95\n",
            "episode:  98 score:  240.09128440637846  average score 232.0 best score 943.95\n",
            "episode:  99 score:  2278.603525446003  average score 252.4 best score 943.95\n",
            "episode:  100 score:  98.02654658118448  average score 252.2 best score 943.95\n",
            "..saving stats\n",
            "episode:  101 score:  240.42487898132177  average score 253.7 best score 943.95\n",
            "episode:  102 score:  374.97975312618064  average score 256.2 best score 943.95\n",
            "episode:  103 score:  72.86695571299248  average score 235.0 best score 943.95\n",
            "episode:  104 score:  33.91012388021584  average score 213.4 best score 943.95\n",
            "episode:  105 score:  50.4110848861707  average score 212.8 best score 943.95\n",
            "episode:  106 score:  313.39044337031123  average score 215.1 best score 943.95\n",
            "episode:  107 score:  590.2892408234833  average score 220.5 best score 943.95\n",
            "episode:  108 score:  92.29086269612374  average score 220.3 best score 943.95\n",
            "episode:  109 score:  248.0655760391278  average score 222.4 best score 943.95\n",
            "episode:  110 score:  165.85966427349794  average score 223.2 best score 943.95\n",
            "episode:  111 score:  319.8873881801189  average score 225.3 best score 943.95\n",
            "episode:  112 score:  146.59757320597598  average score 226.4 best score 943.95\n",
            "episode:  113 score:  199.07473797976758  average score 227.3 best score 943.95\n",
            "episode:  114 score:  353.6896397426652  average score 229.7 best score 943.95\n",
            "episode:  115 score:  712.0463854225967  average score 235.7 best score 943.95\n",
            "episode:  116 score:  1504.991566251378  average score 249.2 best score 943.95\n",
            "episode:  117 score:  198.9020058511869  average score 249.9 best score 943.95\n",
            "episode:  118 score:  75.12616154004125  average score 249.1 best score 943.95\n",
            "episode:  119 score:  1737.7429049010348  average score 264.3 best score 943.95\n",
            "episode:  120 score:  660.2682062306878  average score 269.0 best score 943.95\n",
            "episode:  121 score:  85.75703477645531  average score 269.4 best score 943.95\n",
            "episode:  122 score:  210.24137766342335  average score 269.7 best score 943.95\n",
            "episode:  123 score:  2248.3569282652484  average score 290.1 best score 943.95\n",
            "episode:  124 score:  494.52780755971287  average score 292.5 best score 943.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjGKPUMigo4s"
      },
      "source": [
        "model.load_checkpoint()\n",
        "test=10\n",
        "rewards = []\n",
        "env = Monitor(env, './video', force=True, video_callable=lambda episode: True)\n",
        "for i in range(test):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0.0\n",
        "    #playing one game\n",
        "    while(not done):\n",
        "        action = select_action(state)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "\n",
        "    rewards.append(episode_reward)\n",
        "    print('episode: ', i,'score: ', episode_reward,\n",
        "             ' average score %.1f' % np.mean(rewards[-100:]))\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}